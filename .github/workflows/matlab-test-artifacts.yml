name: MATLAB Engine API Tests with Artifacts

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  matlab-engine-tests:
    runs-on: ubuntu-latest
    
    outputs:
      success_rate: ${{ steps.test_results.outputs.success_rate }}
      total_tests: ${{ steps.test_results.outputs.total_tests }}
      passed_tests: ${{ steps.test_results.outputs.passed_tests }}
      failed_tests: ${{ steps.test_results.outputs.failed_tests }}
      environment: ${{ steps.test_results.outputs.environment }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Set up MATLAB
        uses: matlab-actions/setup-matlab@v2
        with:
          release: R2025a
          
      - name: Install Python dependencies
        run: |
          pip install matlabengine pytest numpy scipy pyyaml
          
      - name: Run MATLAB Engine API Tests
        id: run_tests
        working-directory: src/python
        run: |
          # Run the enhanced pipeline tests
          ./run_matlab_tests.sh || true  # Don't fail the job on test failures
          
      - name: Process Test Results
        id: test_results
        working-directory: src/python
        run: |
          # Process artifacts and set outputs
          python3 upload_artifacts.py
          
          # Extract results from pipeline summary
          if [ -f "test_artifacts/pipeline_summary.json" ]; then
            SUCCESS_RATE=$(jq -r '.test_execution.success_rate' test_artifacts/pipeline_summary.json)
            TOTAL_TESTS=$(jq -r '.test_execution.total_test_suites' test_artifacts/pipeline_summary.json)
            PASSED_TESTS=$(jq -r '.test_execution.passed_suites' test_artifacts/pipeline_summary.json)
            FAILED_TESTS=$(jq -r '.test_execution.failed_suites' test_artifacts/pipeline_summary.json)
            ENVIRONMENT=$(jq -r '.environment' test_artifacts/pipeline_summary.json)
            
            echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
            echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
            echo "passed_tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
            echo "failed_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
            echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
            
            echo "üìä Test Results: $PASSED_TESTS/$TOTAL_TESTS passed ($SUCCESS_RATE% success rate)"
          fi
      
      - name: Upload Test Artifacts
        uses: actions/upload-artifact@v4
        if: always()  # Upload artifacts even if tests fail
        with:
          name: matlab-test-results
          path: |
            src/python/test_artifacts/
          retention-days: 30
          
      - name: Upload JUnit Test Results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: MATLAB Engine API Tests
          path: src/python/test_artifacts/test_results.xml
          reporter: java-junit
          fail-on-error: false
          
      - name: Comment Test Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = 'src/python/test_artifacts/test_summary.md';
            
            if (fs.existsSync(path)) {
              const summary = fs.readFileSync(path, 'utf8');
              const successRate = '${{ steps.test_results.outputs.success_rate }}';
              const statusEmoji = successRate == '100' ? '‚úÖ' : successRate >= '80' ? '‚ö†Ô∏è' : '‚ùå';
              
              const comment = `## ${statusEmoji} MATLAB Engine API Test Results
              
**Success Rate:** ${successRate}% (${{ steps.test_results.outputs.passed_tests }}/${{ steps.test_results.outputs.total_tests }} passed)
**Environment:** ${{ steps.test_results.outputs.environment }}

<details>
<summary>üìÑ Detailed Results</summary>

${summary}

</details>

üîó [View full test artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
      
      - name: Generate Job Summary
        if: always()
        run: |
          echo "# üß™ MATLAB Engine API Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Success Rate | ${{ steps.test_results.outputs.success_rate }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| Tests Passed | ${{ steps.test_results.outputs.passed_tests }}/${{ steps.test_results.outputs.total_tests }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ steps.test_results.outputs.environment }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üìÅ **Artifacts Generated:**" >> $GITHUB_STEP_SUMMARY
          echo "- üìä Test Results JSON" >> $GITHUB_STEP_SUMMARY
          echo "- üìã JUnit XML Report" >> $GITHUB_STEP_SUMMARY
          echo "- üèÅ Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- üìà Consolidated Report" >> $GITHUB_STEP_SUMMARY
          echo "- üè∑Ô∏è Status Badge Data" >> $GITHUB_STEP_SUMMARY
          
      - name: Fail job if success rate below threshold
        if: steps.test_results.outputs.success_rate < 80
        run: |
          echo "‚ùå Test success rate (${{ steps.test_results.outputs.success_rate }}%) is below the 80% threshold"
          exit 1

  # Optional: Badge generation job
  update-badge:
    runs-on: ubuntu-latest
    needs: matlab-engine-tests
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: matlab-test-results
          path: artifacts/
          
      - name: Update README Badge
        run: |
          SUCCESS_RATE="${{ needs.matlab-engine-tests.outputs.success_rate }}"
          
          # Determine badge color
          if (( $(echo "$SUCCESS_RATE >= 95" | bc -l) )); then
            COLOR="brightgreen"
          elif (( $(echo "$SUCCESS_RATE >= 80" | bc -l) )); then
            COLOR="yellow"
          else
            COLOR="red"
          fi
          
          BADGE_URL="https://img.shields.io/badge/MATLAB%20Tests-${SUCCESS_RATE}%25%20passing-${COLOR}"
          
          echo "üìä Generated badge URL: $BADGE_URL"
          
          # Could update README.md here with the new badge URL
          # sed -i "s|https://img.shields.io/badge/MATLAB%20Tests-.*|$BADGE_URL|g" README.md